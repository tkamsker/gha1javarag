# AI Integration Rules

## Provider Support
- Support OpenAI, Ollama, and Anthropic via AI_PROVIDER setting
- Embedding can be done via Weaviate (text2vec-ollama) or client-side
- Use EMBEDDING_PROVIDER to choose embedding pipeline
- All AI calls should be configurable and optional

## Rate Limiting
- Implement rate limiting based on RATE_LIMIT_ENV setting
- Support production, development, test, and emergency modes
- Use appropriate timeouts and retry logic
- Log rate limiting events for monitoring

## Security
- Keep processing local by default
- Require explicit opt-in for external API usage
- Redact secrets from logs
- Use environment variables for API keys
- Never commit sensitive configuration to version control

## Error Handling
- Implement exponential backoff for API failures
- Provide fallback behavior when AI services are unavailable
- Log AI-related errors with appropriate detail levels
- Continue processing even if AI enrichment fails