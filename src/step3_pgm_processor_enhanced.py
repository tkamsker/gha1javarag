"""
Enhanced Step3 PGM Processor with LLM-based Component Classification
Replaces pattern-matching with intelligent content analysis
"""

import json
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass, asdict
import concurrent.futures
import time

# Import the new classification system
from .enhanced_component_classifier import EnhancedComponentClassifier
from .content_analyzer import ComponentType

# Import existing step3_pgm_processor components we'll reuse
from .step3_pgm_processor import (
    Step3PgmProcessor, ComponentAnalysis, ProjectLayerAnalysis
)

class EnhancedStep3PgmProcessor(Step3PgmProcessor):
    """
    Enhanced version of Step3PgmProcessor that uses LLM-based component classification
    instead of simple pattern matching
    """
    
    def __init__(self, config):
        super().__init__(config)
        
        # Add parallel processing attributes if they don't exist
        if not hasattr(self, 'parallel_processing'):
            self.parallel_processing = True
        if not hasattr(self, 'max_workers'):
            self.max_workers = 3
        
        # Initialize enhanced component classifier
        self.enhanced_classifier = EnhancedComponentClassifier(
            config=config,
            llm_client=self.llm_client,
            logger=self.logger
        )
        
        # Override pattern-based classification
        self.use_enhanced_classification = True
        
        self.logger.info("Enhanced Step3PgmProcessor initialized with LLM-based classification")
    
    def analyze_project_components(self, project_name: str, project_data: Dict[str, Any]) -> ProjectLayerAnalysis:
        """
        Enhanced project processing with improved component classification
        """
        self.logger.info(f"Processing project with enhanced classification: {project_name}")
        start_time = time.time()
        
        try:
            # Use enhanced classification instead of pattern matching
            classification_result = self.enhanced_classifier.classify_project_components(project_data)
            
            # Convert to the format expected by the existing processing pipeline
            backend_components = self._convert_to_component_analysis(
                classification_result, ['dao', 'dto', 'service', 'controller']
            )
            frontend_components = self._convert_to_component_analysis(
                classification_result, ['frontend']
            )
            
            # Create analysis object (compatible with existing code)
            analysis = ProjectLayerAnalysis(
                project_name=project_name,
                backend_components=backend_components,
                frontend_components=frontend_components,
                configuration_components=[],  # TODO: handle config components
                data_flows=[],  # Will be populated by parent class methods
                api_endpoints=[],  # Will be populated by parent class methods 
                business_processes=[]  # Will be populated by parent class methods
            )
            
            # Continue with existing processing pipeline (using parent class methods)
            # The analysis object will be further enriched by parent class methods
            requirements_content = self.generate_layered_requirements(analysis)
            
            # Store enhanced data for output generation
            # (Output files will be generated by overridden generate_output_files method)
            
            processing_time = time.time() - start_time
            self.logger.info(f"Enhanced processing completed for {project_name} in {processing_time:.2f}s")
            
            # Store additional enhanced data for later output generation
            self._enhanced_data = {
                'classification_result': classification_result,
                'processing_time': processing_time,
                'quality_metrics': self.enhanced_classifier.validate_classification_quality(classification_result)
            }
            
            # Return the ProjectLayerAnalysis expected by parent class
            return analysis
            
        except Exception as e:
            self.logger.error(f"Enhanced processing failed for {project_name}: {e}")
            # Fallback to original processing
            self.logger.warning(f"Enhanced classification failed for {project_name}, using fallback: {e}")
            return super().analyze_project_components(project_name, project_data)
    
    def _convert_to_component_analysis(self, classification_result: Dict[str, List], 
                                     component_types: List[str]) -> List[ComponentAnalysis]:
        """
        Convert enhanced classification results to ComponentAnalysis objects
        for compatibility with existing code
        """
        components = []
        
        for component_type in component_types:
            type_components = classification_result.get(component_type, [])
            
            for comp_data in type_components:
                # Create ComponentAnalysis with enhanced data (matching parent class structure)
                component = ComponentAnalysis(
                    component_type=comp_data['component_type'],
                    component_name=Path(comp_data['file_path']).stem,
                    file_path=comp_data['file_path'],
                    relationships=comp_data['dependencies'],
                    business_logic=comp_data['business_logic'] if isinstance(comp_data['business_logic'], list) else [comp_data['business_logic']],
                    endpoints=comp_data['api_endpoints'],
                    weaviate_enrichment={}  # Will be filled by enrichment step
                )
                components.append(component)
        
        return components
    
    def generate_enhanced_output_files(self, analysis: ProjectLayerAnalysis, 
                                     requirements_content: str,
                                     classification_result: Dict[str, List]) -> None:
        """
        Generate enhanced output files with classification quality information
        """
        project_dir = self.pgm_output_dir / analysis.project_name
        project_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate standard output files (using parent class method)
        self.generate_output_files(analysis)
        
        # Generate enhanced classification report
        self._generate_classification_quality_report(project_dir, classification_result)
        
        # Generate component analysis JSON with enhanced data
        self._generate_enhanced_component_json(project_dir, classification_result)
    
    def _generate_classification_quality_report(self, project_dir: Path, 
                                              classification_result: Dict[str, List]) -> None:
        """Generate quality report for the enhanced classification"""
        
        quality_metrics = self.enhanced_classifier.validate_classification_quality(classification_result)
        
        report_content = f"""# Enhanced Classification Quality Report

Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}

## Classification Statistics

- **Total Components Analyzed**: {quality_metrics['total_components']}
- **Average Confidence**: {quality_metrics['avg_confidence']:.2f}
- **Low Confidence Components**: {quality_metrics['low_confidence_count']}
- **Files Excluded**: {quality_metrics.get('exclusion_count', 0)}
- **Analysis Failures**: {quality_metrics.get('failure_count', 0)}

## Component Distribution

"""
        
        # Add component type breakdown
        for component_type, components in classification_result.items():
            if component_type != 'statistics' and components:
                report_content += f"- **{component_type.upper()}**: {len(components)} components\\n"
        
        report_content += f"""
## Quality Assessment

**Exclusion Ratio**: {quality_metrics['exclusion_ratio']:.2f} (target: <0.8)
**Failure Ratio**: {quality_metrics['failure_ratio']:.2f} (target: <0.1)
**Average Confidence**: {quality_metrics['avg_confidence']:.2f} (target: >0.8)

## Recommendations

"""
        
        if quality_metrics['recommendations']:
            for rec in quality_metrics['recommendations']:
                report_content += f"- {rec}\\n"
        else:
            report_content += "- No issues detected - classification quality is good\\n"
        
        # Low confidence components
        if quality_metrics['low_confidence_count'] > 0:
            report_content += f"""
## Low Confidence Components

The following components were classified with confidence < 0.7:

"""
            
            for component_type, components in classification_result.items():
                if component_type == 'statistics':
                    continue
                    
                low_conf_components = [c for c in components if c.get('confidence', 1.0) < 0.7]
                if low_conf_components:
                    report_content += f"### {component_type.upper()}\\n"
                    for comp in low_conf_components:
                        report_content += f"- {comp['file_path']} (confidence: {comp['confidence']:.2f}) - {comp['purpose']}\\n"
                    report_content += "\\n"
        
        # Save report
        with open(project_dir / 'classification_quality_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
    
    def _generate_enhanced_component_json(self, project_dir: Path, 
                                        classification_result: Dict[str, List]) -> None:
        """Generate detailed JSON file with all classification data"""
        
        # Create comprehensive component data
        enhanced_data = {
            'generation_info': {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'processor': 'EnhancedStep3PgmProcessor',
                'classification_method': 'LLM-based content analysis'
            },
            'statistics': classification_result.get('statistics', {}),
            'components': {}
        }
        
        # Organize components by type with full details
        for component_type, components in classification_result.items():
            if component_type != 'statistics' and components:
                enhanced_data['components'][component_type] = []
                
                for comp in components:
                    # Add full component details
                    comp_detail = {
                        'file_path': comp['file_path'],
                        'component_type': comp['component_type'],
                        'confidence': comp['confidence'],
                        'analysis': {
                            'purpose': comp['purpose'],
                            'business_logic': comp['business_logic'],
                            'api_endpoints': comp['api_endpoints'],
                            'dependencies': comp['dependencies'],
                            'annotations': comp['annotations'],
                            'key_methods': comp['key_methods']
                        }
                    }
                    
                    if 'error_message' in comp:
                        comp_detail['error_message'] = comp['error_message']
                    
                    enhanced_data['components'][component_type].append(comp_detail)
        
        # Save enhanced JSON
        with open(project_dir / 'enhanced_component_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(enhanced_data, f, indent=2, ensure_ascii=False)
    
    def fallback_to_original_processing(self, project_name: str, project_data: Dict[str, Any], 
                                      error: str) -> Dict[str, Any]:
        """
        Fallback to original pattern-based processing if enhanced classification fails
        """
        self.logger.warning(f"Enhanced classification failed for {project_name}, falling back to original processing: {error}")
        
        # Temporarily disable enhanced classification
        self.use_enhanced_classification = False
        
        try:
            # Use parent class processing
            result = super().process_project(project_name, project_data)
            result['fallback_used'] = True
            result['fallback_reason'] = error
            return result
        except Exception as fallback_error:
            self.logger.error(f"Fallback processing also failed for {project_name}: {fallback_error}")
            return {
                'project_name': project_name,
                'error': f"Both enhanced and fallback processing failed: {error} | {fallback_error}",
                'analysis': None,
                'requirements_content': "# Processing Failed\\n\\nBoth enhanced and fallback processing encountered errors.",
                'processing_time': 0
            }
        finally:
            # Re-enable enhanced classification
            self.use_enhanced_classification = True
    
    def classify_component(self, file_path: str, file_content: str, 
                          enhanced_analysis: Dict[str, Any]) -> str:
        """
        Override parent method to use enhanced classification when available
        """
        if self.use_enhanced_classification:
            # This method shouldn't be called directly when using enhanced classification
            # But provide fallback for compatibility
            self.logger.warning(f"classify_component called directly for {file_path}, using fallback")
        
        # Use original classification as fallback
        return super().classify_component(file_path, file_content, enhanced_analysis)
    
    
    def generate_output_files(self, analysis: ProjectLayerAnalysis) -> None:
        """Override parent method to add enhanced output files"""
        # First call parent method to generate standard files
        super().generate_output_files(analysis)
        
        # Then generate enhanced files if we have enhanced data
        if hasattr(self, '_enhanced_data'):
            project_dir = self.pgm_output_dir / analysis.project_name
            self._generate_classification_quality_report(project_dir, self._enhanced_data['classification_result'])
            self._generate_enhanced_component_json(project_dir, self._enhanced_data['classification_result'])
    
    def _generate_enhanced_summary(self, results: List[Dict[str, Any]], total_time: float) -> None:
        """Generate enhanced summary with quality metrics"""
        
        successful_results = [r for r in results if 'error' not in r]
        failed_results = [r for r in results if 'error' in r]
        fallback_results = [r for r in successful_results if r.get('fallback_used', False)]
        
        # Aggregate statistics
        total_components = 0
        total_backend = 0
        total_frontend = 0
        avg_confidence = 0.0
        confidence_count = 0
        
        for result in successful_results:
            if 'quality_metrics' in result:
                metrics = result['quality_metrics']
                total_components += metrics.get('total_components', 0)
                if 'avg_confidence' in metrics:
                    avg_confidence += metrics['avg_confidence']
                    confidence_count += 1
            
            if 'analysis' in result and result['analysis']:
                analysis = result['analysis']
                total_backend += len(analysis.get('backend_components', []))
                total_frontend += len(analysis.get('frontend_components', []))
        
        if confidence_count > 0:
            avg_confidence /= confidence_count
        
        # Generate summary content
        summary_content = f"""# Enhanced Step3-PGM Processing Summary

Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}
Processing Method: **LLM-based Content Analysis**
Total Processing Time: **{total_time:.2f} seconds**

## Processing Results

- **Total Projects**: {len(results)}
- **Successful**: {len(successful_results)}
- **Failed**: {len(failed_results)}
- **Used Fallback**: {len(fallback_results)}

## Component Analysis Statistics

- **Total Components Classified**: {total_components:,}
- **Backend Components**: {total_backend:,}
  - DAO/Repository layers  
  - DTO/Entity models
  - Service/Business logic
  - Controller/API endpoints
- **Frontend Components**: {total_frontend:,}
  - UI Views and templates
  - JavaScript/TypeScript code
  - Stylesheets and assets

## Quality Metrics

- **Average Classification Confidence**: {avg_confidence:.2f}
- **Enhanced Classification**: {len([r for r in successful_results if not r.get('fallback_used', False)])} projects
- **Pattern Fallback Used**: {len(fallback_results)} projects

## Enhanced Features Delivered

✅ **Content-Based Classification**: Files analyzed by actual content, not just filename patterns
✅ **LLM-Powered Analysis**: Intelligent understanding of code structure and purpose
✅ **Quality Metrics**: Confidence scores and validation for each classification
✅ **Detailed Component Analysis**: Business logic, API endpoints, and dependencies extracted
✅ **Fallback Safety**: Automatic fallback to pattern-matching if enhanced analysis fails

## Project Processing Details

"""
        
        # Add per-project details
        for result in results:
            project_name = result['project_name']
            processing_time = result.get('processing_time', 0)
            
            if 'error' in result:
                summary_content += f"### ❌ {project_name}\\n"
                summary_content += f"- **Status**: Failed\\n"
                summary_content += f"- **Error**: {result['error']}\\n\\n"
            else:
                summary_content += f"### ✅ {project_name}\\n"
                summary_content += f"- **Processing Time**: {processing_time:.2f}s\\n"
                
                if result.get('fallback_used'):
                    summary_content += f"- **Method**: Pattern-based (fallback)\\n"
                    summary_content += f"- **Fallback Reason**: {result.get('fallback_reason', 'Unknown')}\\n"
                else:
                    summary_content += f"- **Method**: Enhanced LLM classification\\n"
                
                if 'quality_metrics' in result:
                    metrics = result['quality_metrics']
                    summary_content += f"- **Components**: {metrics.get('total_components', 0)}\\n"
                    summary_content += f"- **Avg Confidence**: {metrics.get('avg_confidence', 0):.2f}\\n"
                    
                    if metrics.get('recommendations'):
                        summary_content += f"- **Recommendations**: {len(metrics['recommendations'])} quality issues\\n"
                
                summary_content += "\\n"
        
        # Add recommendations section
        summary_content += """
## Next Steps

1. **Review Classification Quality Reports**: Check individual project quality reports for low-confidence classifications
2. **Validate High-Impact Components**: Manually verify DAO/Service classifications for critical business components  
3. **Compare with Pattern-Based Results**: Use fallback results to identify areas where enhanced classification differs significantly
4. **Iterate on Prompts**: If average confidence is low, consider refining LLM analysis prompts

## File Locations

- **Project Analysis**: `{OUTPUT_DIR}/requirements/pgm/projects/{project_name}/`
- **Quality Reports**: `classification_quality_report.md` in each project directory
- **Enhanced Data**: `enhanced_component_analysis.json` with full classification details
- **Standard Output**: Compatible with existing step3-pgm structure for requirements and traceability

Enhanced classification provides significantly more accurate component identification compared to pattern-matching,
leading to higher quality requirements documentation and better architectural insights.
"""
        
        # Save summary
        summary_path = self.pgm_output_dir / '_enhanced_pgm_summary.md'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        self.logger.info(f"Enhanced summary generated: {summary_path}")